{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name: Common airline functions\n",
    "# Purpose: Central file containing airline functions that I use nearly daily on airline studies\n",
    "# Creator: Alex Deshowitz\n",
    "# Project: Applicable to airline research and commonly performed actions on airline data\n",
    "# Date created: 1/23/2018\n",
    "# Date updated: 3/5/2020 \n",
    "\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for use in the notebook:\n",
    "\n",
    "def coterminal_replacement ( airport_code_array ):\n",
    "    \n",
    "    ''' Function that replaces cities that we want to view as commercially \"the same\" \n",
    "    within our analysis in order to create one large coterminal from multiple airports '''\n",
    "    \n",
    "    city_code =  np.where(airport_code_array.isin(['IAH','HOU']), 'IAH', \n",
    "                          np.where(airport_code_array.isin(['MDW','ORD']),'ORD',\n",
    "                          np.where(airport_code_array.isin(['DFW','DAL']), 'DFW',\n",
    "                          np.where(airport_code_array.isin(['LAX','SNA', 'ONT','BUR', 'LGB']), 'LAX', \n",
    "                          np.where(airport_code_array.isin(['JFK','LGA','EWR']), 'NYC', \n",
    "                          np.where(airport_code_array.isin(['DCA','IAD']), 'DCA',\n",
    "                          np.where(airport_code_array.isin(['FLL','MIA']), 'MIA', airport_code_array)))))))\n",
    "    return city_code\n",
    "\n",
    "def three_domestic_sisters_coterminal_replacement (airport_code_array ):\n",
    "    \n",
    "    '''Returns an array of the 3-coterminaled sister airport/cities in the US'''\n",
    "    \n",
    "    return np.where(airport_code_array.isin(['IAH','HOU']), \n",
    "                    'IAH', np.where(airport_code_array.isin(['MDW','ORD']),\n",
    "                    'ORD', np.where(airport_code_array.isin(['DFW','DAL']), \n",
    "                    'DFW', airport_code_array)))\n",
    "\n",
    "def airline_code_fix(carrier_array): \n",
    "    \n",
    "    '''Function that overrides carrier codes in the dataset for carriers that have combined\n",
    "    (via merger or acquisition) during the analysis period'''\n",
    "    \n",
    "    \n",
    "    airline_code = np.where(carrier_array.isin(['WN','FL']), 'WN',\n",
    "                   np.where(carrier_array.isin(['AS','VX']), 'AS',\n",
    "                   np.where(carrier_array.isin(['UA','CO']), 'UA',\n",
    "                   np.where(carrier_array.isin(['DL','NW']), 'DL',\n",
    "                   np.where(carrier_array.isin(['AA','US','HP']), 'AA',carrier_array)))))\n",
    "    return airline_code\n",
    "\n",
    "def airline_code_category(carrier_array):\n",
    "    \n",
    "    '''Returns array or Series containing the mapped airline category that each airline fits into - NOTE: only for domestic US cxrs right now'''\n",
    "    \n",
    "    \n",
    "    legacy = ['AA','UA','DL', 'NW','US', 'CO'] \n",
    "    lcc = ['B6', 'VX', 'AS', 'WN', 'FL']\n",
    "    ulcc = ['NK', 'F9', 'G4']\n",
    "    \n",
    "    airline_group = np.where(carrier_array.isin(legacy), 'Legacy',\n",
    "                    np.where(carrier_array.isin(lcc), 'LCC',\n",
    "                    np.where(carrier_array.isin(ulcc), 'ULCC', 'Other')))\n",
    "    return airline_group\n",
    "\n",
    "def rt_market(orig_array, dest_array):\n",
    "    \n",
    "    '''Returns an array of half alpha markets from an orig and dest array set'''\n",
    "    \n",
    "    market_array = np.where(orig_array < dest_array, orig_array + '-' + dest_array, dest_array + '-' + orig_array)\n",
    "    return market_array\n",
    "\n",
    "def dir_market(orig_array, dest_array):\n",
    "    \n",
    "    '''Returns an array of directional-one-way markets from an orig and dest array set'''\n",
    "    \n",
    "    market_array = orig_array + '-' + dest_array\n",
    "    return market_array\n",
    "\n",
    "def otp_ind(dot_delay_mins, ontime_cutoff = 14, ontime_minority_class = 1 ):\n",
    "    \n",
    "    '''Returns boolean array from dot delay minutes for identification of \"DOT Late\" flights; takes df['col'] as argument;\n",
    "    ontime_minority_class argument added for classification task target variable prep'''\n",
    "    \n",
    "    if ontime_minority_class == 1:\n",
    "        ontime = np.where(dot_delay_mins <= ontime_cutoff, 1, 0)\n",
    "    \n",
    "    else:\n",
    "        ontime = np.where(dot_delay_mins <= ontime_cutoff, 0, 1)\n",
    "        \n",
    "    return ontime\n",
    "\n",
    "def split_diio_date_qtr(df, column):\n",
    "    \n",
    "    '''Returns an array of the quarters in a dataset from the poorly formatted Diio \"date\" array'''\n",
    "    \n",
    "    \n",
    "    new_col = df[column].str.split(' ', n = 1, expand = True)[0]\n",
    "    return new_col\n",
    "\n",
    "def split_diio_date_yr(df, column):\n",
    "    \n",
    "    '''Returns an array of the quarters in a dataset from the poorly formatted Diio \"date\" array'''\n",
    "    \n",
    "    \n",
    "    new_col = df[column].str.split(' ', n = 1, expand = True)[1]\n",
    "    return new_col\n",
    "\n",
    "def fix_columns(dataframe):\n",
    "    \n",
    "    '''function that takes a list of columns and modifies them to be easier to read -- assign to df.columns'''\n",
    "    \n",
    "    column_string_replace = ['\\n','@',' ','__', '/', '-']\n",
    "\n",
    "    columns = dataframe.columns\n",
    "\n",
    "    columns = columns.map(lambda x: x.strip())\n",
    "    columns = columns.map(lambda x : x.lower())\n",
    "\n",
    "    for string in column_string_replace:\n",
    "        columns = columns.map(lambda x : x.replace(string, '_') if isinstance (x, (str, bytes)) else x)\n",
    "\n",
    "    return columns\n",
    "\n",
    "def left(s, amount):\n",
    "    \n",
    "    '''Returns the left n characters of a string - use map and apply for an array '''\n",
    "    \n",
    "    \n",
    "    return s[:amount]\n",
    "\n",
    "def right(s, amount):\n",
    "    \n",
    "    '''Returns the right n characters of a string - use map and apply for an array '''\n",
    "\n",
    "    \n",
    "    return s[-amount:]\n",
    "\n",
    "def mid(s, offset, amount):\n",
    "    \n",
    "    '''Returns the mid n characters of a string - use map and apply for an array'''\n",
    "    \n",
    "    return s[offset:offset+amount]\n",
    "\n",
    "def combine_sked_files(replaceable_path, combined_file_path, delimiter = 'None', header = 2):\n",
    "    '''function that combines the files in the sked files from Diio - easy to manipulate to convert different file types'''\n",
    "\n",
    "    start = tm.time()\n",
    "    \n",
    "    counter = 1\n",
    "    \n",
    "    for file in glob.glob(replaceable_path):\n",
    "        \n",
    "        if counter == 1:\n",
    "            print(counter, \":\")\n",
    "            print('reading file: ', file)\n",
    "            df = pd.read_csv(file, delimiter = delimiter, header = header)\n",
    "            df.columns = fix_columns(df)\n",
    "            \n",
    "            # cut out the null rows NOTE:SPECIFIC to Diio files:\n",
    "            df = df.iloc[0:np.where(df.iloc[:,0].isnull().values == True)[0].min(),:]\n",
    "            \n",
    "            df['file_index'] = counter\n",
    "            df.to_csv(combined_file_path, index = False)\n",
    "            print('generated new file: ', combined_file_path)\n",
    "            \n",
    "            del df\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(counter, \":\")\n",
    "            print('reading file: ', file)\n",
    "            \n",
    "            df = pd.read_csv(file, delimiter = delimiter, header = header)\n",
    "            df.columns = fix_columns(df)\n",
    "            # cut out the null rows NOTE:SPECIFIC to Diio files:\n",
    "            df = df.iloc[0:np.where(df.iloc[:,0].isnull().values == True)[0].min(),:]\n",
    "            \n",
    "            df['file_index'] = counter\n",
    "            df.to_csv(combined_file_path, mode = 'a', header = False, index = False)\n",
    "            print('appended data from: ', file, 'into: ', combined_file_path)\n",
    "            \n",
    "            \n",
    "            del df\n",
    "        \n",
    "        counter += 1\n",
    "    end = tm.time()\n",
    "    \n",
    "    print('Files completed combining in : ', round((end - start) / 60), ' minutes')\n",
    "    \n",
    "def HHI (dataframe, level_of_detail, market_measure): \n",
    "    \n",
    "    \"\"\"Function that calculates and returns the HHI of each market -- NOTE THAT THIS ONLY PERTAINS TO OVERALL MARKET HHI - NO ADDL DIMENSIONALITY\"\"\"\n",
    "    \n",
    "    total_market = dataframe.groupby(by = level_of_detail)[market_measure].transform(sum)\n",
    "    market_share = (((dataframe[market_measure] / total_market) * 100)**2) \n",
    "    \n",
    "    subframe = pd.concat([dataframe[level_of_detail], market_share], axis = 1, ignore_index = True )\n",
    "    \n",
    "    subframe.columns = [level_of_detail, 'market_share']\n",
    "    \n",
    "    hhi = subframe.groupby(by = level_of_detail)['market_share'].transform(sum)\n",
    "    \n",
    "    return hhi\n",
    "\n",
    "def calculate_haversine(lat_1 = 32.845945, lon_1 = -96.850876, \n",
    "                        lat_2 = 29.645417, lon_2 = -95.278888):\n",
    "    \n",
    "    ''' Function that calculates the great-circle (Haversine) distance between 2 locations in miles'''\n",
    "    R = 6373.0 # radius of the earth\n",
    "    \n",
    "    lat1 = radians(lat_1)\n",
    "    lon1 = radians(lon_1)\n",
    "    lat2 = radians(lat_2)\n",
    "    lon2 = radians(lon_2)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    distance = distance * 0.621371 # convert to miles\n",
    "    return distance  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
